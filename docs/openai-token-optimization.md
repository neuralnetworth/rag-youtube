# OpenAI Token Optimization for RAG Pipelines (2025)

This document outlines optimal token settings for OpenAI models in RAG applications, based on research conducted in January 2025.

## Executive Summary

**Best Practice**: For OpenAI models, `max_tokens` can be safely set OR omitted. Unlike Gemini, there are no known bugs with either approach.

## Model-Specific Parameter Differences

### üéØ GPT-4.1 (Generative Models)
- **Parameter**: `max_tokens`
- **Maximum**: 32,768 output tokens
- **Default when omitted**: "inf" (capped by model limit)
- **Temperature**: Fully supported
- **Recommendation**: Safe to set 1000-2000 for RAG responses

### üß† o3 Series (Reasoning Models)
- **Parameter**: `max_completion_tokens` (NOT `max_tokens`)
- **Maximum**: 100,000 output tokens
- **Temperature**: NOT supported (causes API errors)
- **Hidden tokens**: Uses internal reasoning tokens not visible in output
- **Recommendation**: Always use `max_completion_tokens`, never `max_tokens`

## Default Behavior Analysis

### When `max_tokens` is Omitted

1. **Theory**: Defaults to infinity ("inf")
2. **Practice**: Limited by:
   - Model's maximum output capacity
   - Remaining context window after input tokens
3. **No bugs**: Unlike Gemini, works reliably

### Model Output Limits

| Model | Max Output Tokens | Context Window | Parameter Name |
|-------|------------------|----------------|----------------|
| GPT-4.1 | 32,768 | 1M | `max_tokens` |
| GPT-4o | 16,384 | 128K | `max_tokens` |
| GPT-4 | 4,096 | 128K | `max_tokens` |
| o3 | 100,000 | 200K | `max_completion_tokens` |
| o3-mini | 100,000 | 128K | `max_completion_tokens` |

## Implementation Guidelines

### ‚úÖ Safe Configuration for GPT-4.1

```python
def _get_model_params(self, **kwargs) -> Dict[str, Any]:
    """Get model-specific parameters for OpenAI."""
    model = kwargs.get('model', self.model)
    params = {
        'model': model,
        'max_tokens': kwargs.get('max_tokens', 1000)  # Safe default for RAG
    }
    
    # Only add temperature if explicitly provided
    if 'temperature' in kwargs:
        params['temperature'] = kwargs['temperature']
    
    # o3 models use different parameter names and don't support temperature
    if model.startswith("o3"):
        params['max_completion_tokens'] = params.pop('max_tokens')
        # Remove temperature for o3 models as it's not supported
        params.pop('temperature', None)
    
    return params
```

### üöÄ RAG-Specific Recommendations

For RAG pipelines with OpenAI:

1. **Option 1: Set explicit limit**
   ```python
   kwargs = {"max_tokens": 1000}  # Good for concise answers
   ```

2. **Option 2: Let model decide**
   ```python
   kwargs = {}  # Model uses available context
   ```

Both options work well - choose based on your needs:
- **Set limit**: For predictable response lengths and cost control
- **No limit**: For complex answers that may need more tokens

## Comparison with Gemini

### Key Differences

| Aspect | OpenAI | Gemini |
|--------|---------|---------|
| Omit max_tokens | ‚úÖ Works fine | ‚ùå Can cause empty responses |
| Set max_tokens | ‚úÖ Works as expected | ‚ö†Ô∏è May cause issues with 2.5 Pro |
| Best practice | Either approach | Avoid setting max_tokens |

## Cost Considerations

### Token Usage Impact

- **With limit**: Predictable costs, may truncate complex answers
- **Without limit**: Higher potential costs, complete answers
- **Recommendation**: Set reasonable limits (1000-2000) for production RAG

### Reasoning Models (o3)

- **Hidden cost**: Internal reasoning tokens (not visible)
- **Example**: 200-token answer might use 10,000 reasoning tokens
- **Budget accordingly**: o3 models are more expensive per query

## Troubleshooting

### Common Issues

1. **Wrong parameter for o3**:
   ```python
   # ‚ùå Wrong
   {"model": "o3", "max_tokens": 1000}
   
   # ‚úÖ Correct
   {"model": "o3", "max_completion_tokens": 1000}
   ```

2. **Temperature with o3**:
   ```python
   # ‚ùå Causes API error
   {"model": "o3", "temperature": 0.7}
   
   # ‚úÖ Correct - no temperature
   {"model": "o3"}
   ```

## Best Practices Summary

### For RAG Applications

1. **GPT-4.1 (default)**: Safe to set `max_tokens` to 1000-2000
2. **Cost control**: Always set limits in production
3. **Quality**: Test with/without limits for your use case
4. **o3 models**: Use only for complex reasoning, not general RAG

### Parameter Checklist

- [ ] Use correct parameter name (`max_tokens` vs `max_completion_tokens`)
- [ ] Remove temperature for o3 models
- [ ] Set reasonable limits for cost control
- [ ] Test thoroughly with your specific prompts

---

*Document created: January 2025*  
*Last updated: January 10, 2025*  
*Research basis: OpenAI Developer Forums, Official API Documentation*